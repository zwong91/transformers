{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fa5d73a-f8f5-40c3-a4c6-d2350ac3d25b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cb8d161-ae6e-487f-9f4d-66725f84df89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bitsandbytes：专为量化设计的库，重点在于减少大语言模型（尤其是在GPU上）的内存占用。\n",
    "# peft：用于将LoRA适配器集成到大语言模型（LLMs）中。\n",
    "# trl：该库包含一个SFT（监督微调）类，用于辅助微调模型。\n",
    "# accelerate和xformers：这些库用于提高模型的推理速度，从而优化其性能。\n",
    "# wandb：该工具作为一个监控平台，用于跟踪和观察训练过程。\n",
    "# datasets：与Hugging Face一起使用，该库便于加载数据集。\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    TextStreamer,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import os, wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a42bb5d9-31d1-4719-bec6-7e61bf1bd9f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbe1bb15-91c6-4d8e-b24c-005ae706972a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1957a81-b928-4aa2-9139-6a476c9f339e",
   "metadata": {},
   "source": [
    "## 1. 加载模型和Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f91d603-5177-45ac-86ae-182d158319b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 预训练模型\n",
    "model_name = \"/workspace/runpod-tmp/model/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "# 数据集名称\n",
    "dataset_name = \"scooterman/guanaco-llama3-1k\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87ad9fb8-0fdd-4069-a1a0-e2fe510c2bd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff230c69cae142518d4c7cd6d8197e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 加载预训练模型和tokenizer\n",
    "\n",
    "# 量化配置\n",
    "# https://huggingface.co/docs/transformers/en/main_classes/quantization#transformers.BitsAndBytesConfig\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True, # 模型将以4位量化格式加载\n",
    "    bnb_4bit_quant_type = \"nf4\", # 指定4位量化的类型为 nf4 \n",
    "    bnb_4bit_compute_dtype = torch.float16, # 计算数据类型 \n",
    "    bnb_4bit_use_double_quant = False, # 表示不使用双重量化\n",
    ")\n",
    "\n",
    "# 模型加载\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = {\"\": 0} # 将模型加载到设备0（通常是第一个GPU）\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model) \n",
    "\n",
    "# tokenizer 加载\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True # 在生成序列时会自动添加结束标记\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "321be706-7129-4eda-9206-691aebb8ba13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|start_header_id|>user<|end_header_id|>{{Me gradué hace poco de la carrera de medicina ¿Me podrías aconsejar para conseguir rápidamente un puesto de trabajo?}}<|eot_id|><|start_header_id|>assistant<|end_header_id|>{{Esto vale tanto para médicos como para cualquier otra profesión tras finalizar los estudios aniversarios y mi consejo sería preguntar a cuántas personas haya conocido mejor. En este caso, mi primera opción sería hablar con otros profesionales médicos, echar currículos en hospitales y cualquier centro de salud. En paralelo, trabajaría por mejorar mi marca personal como médico mediante un blog o formas digitales de comunicación como los vídeos. Y, para mejorar las posibilidades de encontrar trabajo, también participaría en congresos y encuentros para conseguir más contactos. Y, además de todo lo anterior, seguiría estudiando para presentarme a las oposiciones y ejercer la medicina en el sector público de mi país.}}<|eot_id|>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载数据集\n",
    "\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "dataset[\"text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8d7af1-be7c-423f-96ce-f85c91d66d51",
   "metadata": {},
   "source": [
    "## 2.wandb配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b7063b7-555e-49ca-8eb0-31e4b37b9fea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtommytang\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 监控\n",
    "# 需要在WandB官网注册账号\n",
    "\n",
    "wandb.login(key=\"695dbbe83ed95db416651f66e8f5d5488f9146b7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ec5c9b6-473a-4430-b933-6d8c750cf0fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjongs-un\u001b[0m (\u001b[33mjongs-un-Personal\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/transformers/Others/03-Llama3-finetune/2_LoRA/02_案例实战/wandb/run-20241023_075120-m4qckv90</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jongs-un-Personal/finetune%20Llama-3.2-11B-Vision-Instruct/runs/m4qckv90' target=\"_blank\">royal-pyramid-1</a></strong> to <a href='https://wandb.ai/jongs-un-Personal/finetune%20Llama-3.2-11B-Vision-Instruct' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jongs-un-Personal/finetune%20Llama-3.2-11B-Vision-Instruct' target=\"_blank\">https://wandb.ai/jongs-un-Personal/finetune%20Llama-3.2-11B-Vision-Instruct</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jongs-un-Personal/finetune%20Llama-3.2-11B-Vision-Instruct/runs/m4qckv90' target=\"_blank\">https://wandb.ai/jongs-un-Personal/finetune%20Llama-3.2-11B-Vision-Instruct/runs/m4qckv90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project=\"finetune Llama-3.2-11B-Vision-Instruct\",\n",
    "    job_type = \"training\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5be707db-4d63-4b9b-9742-423d4a345f80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 计算训练参数量\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"训练参数量 : {trainable_params} || 总的参数量 : {all_param} || 训练参数量占比%: {100 * (trainable_params / all_param):.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebe0889-626b-491e-b9f5-3040cc5ab266",
   "metadata": {},
   "source": [
    "## 3. LoRA与训练超参配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1f2140b-94a7-4494-a007-78ecbe7e54af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r = 8,\n",
    "    lora_alpha = 16, # 小技巧：把α值设置成rank值的两倍\n",
    "    # scaling = alpha / r # LoRA 权重的值越大，影响就越大。\n",
    "    # weight += (lora_B @ lora_A) * scaling\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\",\n",
    "    # [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\", \"down_proj\", \"embed_tokens\", \"lm_head\"]\n",
    "    target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8111d14b-70b4-49b2-9668-c8ec54584758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 训练超参\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir = \"/workspace/runpod-tmp/output\",\n",
    "    num_train_epochs = 5,\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps = 2, # 梯度累积步数为2，即每2步更新一次梯度。有助于在显存有限的情况下使用较大的有效批次大小。\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    save_steps = 100, # 每100步保存一次模型 \n",
    "    logging_steps = 30,\n",
    "    learning_rate = 2e-4,\n",
    "    weight_decay = 0.001, # 权重衰减系数，用于L2正则化，帮助防止过拟合。\n",
    "    fp16 = False,\n",
    "    bf16 = False,\n",
    "    max_grad_norm = 0.3, # 最大梯度范数，用于梯度裁剪，防止梯度爆炸。\n",
    "    max_steps = -1, # 最大训练步数为-1，表示没有限制。\n",
    "    warmup_ratio = 0.3, # 预热阶段的比例。在训练开始时，学习率会逐渐升高，预热比例为0.3表示前30%的训练步骤用于预热。\n",
    "    group_by_length = True, # 按序列长度分组，以提高训练效率。\n",
    "    lr_scheduler_type = \"linear\", # 表示使用线性学习率调度。\n",
    "    report_to = \"wandb\", # tensorboard\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3254d9d5-5cca-4b03-b16b-9c6c81cc8c8c",
   "metadata": {},
   "source": [
    "## 4. 模型微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efda6ecb-182a-4bfc-bc61-58f16f6f2455",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/transformers/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/root/miniconda3/envs/transformers/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/transformers/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0eca3f03d8e46f88ea4ae7d911eb660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SFT超参\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    peft_config = peft_config,\n",
    "    tokenizer = tokenizer,\n",
    "    dataset_text_field=\"text\",\n",
    "    args = training_arguments,\n",
    "    packing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f768663-e041-4cb6-b9c8-cc675d64f91a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/root/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 22/625 03:20 < 1:40:51, 0.10 it/s, Epoch 0.17/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 开始训练\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a2d593-e13f-42f9-8e80-9e09caadebaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# 计算可训练参数量\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293c4c98-29bb-4c24-a719-3530798088c7",
   "metadata": {},
   "source": [
    "## 5. 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017e145e-9445-4b6a-827e-4db15cdaaba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 保存微调模型\n",
    "\n",
    "trainer.model.save_pretrained(\"/workspace/runpod-tmp/model/lora_model\")\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "model.config.use_cache = True\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de1c28-ab48-42b8-98e6-b7fd76cb8cc0",
   "metadata": {},
   "source": [
    "## 6. 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7f7857-d793-484e-ba21-2177263a2b96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# base模型测试\n",
    "\n",
    "def stream(user_input):\n",
    "    device = \"cuda:0\"\n",
    "    system_prompt = 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n'\n",
    "    B_INST, E_INST = \"### Instruction:\\n\", \"### Response:\\n\"\n",
    "    prompt = f\"{system_prompt}{B_INST}{user_input.strip()}\\n\\n{E_INST}\"\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e4be79-5ce9-4bdd-b081-2ec689ca2faf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stream(\"Tell me something about the Great Wall.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab8b61b-b0a1-4aea-b7d6-3199940b7afe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    TextStreamer,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import os, wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7534a26-c389-40d7-a596-a0c598ff87e4",
   "metadata": {},
   "source": [
    "## 7. 模型合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6322f2f7-5917-4fbf-a530-e6838ea8242f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 预训练模型\n",
    "model_name = \"/workspace/runpod-tmp/model/Meta-Llama-3.2-11B-Vision-Instuct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6117311-6eb6-4fca-91f6-6a4506fb0325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 合并 base model 与 lora model\n",
    "# https://huggingface.co/docs/trl/main/en/use_model#use-adapters-peft\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, low_cpu_mem_usage=True,\n",
    "    return_dict=True,torch_dtype=torch.float16,\n",
    "    device_map= {\"\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6cbce1-6c68-4624-a964-1235a3693222",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_model = PeftModel.from_pretrained(base_model, \"/workspace/runpod-tmp/model/lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68696e4a-d578-4dd3-b505-481f20afbf0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 模型合并\n",
    "\n",
    "merged_model = new_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560d282d-a367-4e4b-a644-4b75fb7bcd02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cecd9c-77c7-49cc-8b61-947180b55dfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"Tell me something about the Great Wall.\"\n",
    "device = \"cuda:0\"\n",
    "system_prompt = 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n'\n",
    "B_INST, E_INST = \"### Instruction:\\n\", \"### Response:\\n\"\n",
    "prompt = f\"{system_prompt}{B_INST}{user_input.strip()}\\n\\n{E_INST}\"\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "_ = merged_model.generate(**inputs, streamer=streamer, max_new_tokens=128, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8263ad-ad3c-4099-83bc-fcb5c074c563",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda & pytorch(transformers)",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
