{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be4f4b15-318a-4f4f-8a66-d91385906952",
   "metadata": {},
   "source": [
    "## 1. 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17d447a5-22dc-49b7-8d26-16b99bf721eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 1. 加载数据集\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      5\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgem/viggo\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgem/viggo\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"gem/viggo\", split=\"train\")\n",
    "eval_dataset = load_dataset(\"gem/viggo\", split=\"validation\")\n",
    "test_dataset = load_dataset(\"gem/viggo\", split=\"test\")\n",
    "\n",
    "print(train_dataset)\n",
    "print(eval_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a421faab-4303-4a9f-8aa0-8592e269c687",
   "metadata": {},
   "source": [
    "## 2. 加载基模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b32d94-3988-4251-a353-21ff300076ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"\"\n",
    "\n",
    "# 量化配置\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_type = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id,\n",
    "                                             quantization_config = bnb_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a0b74d-bb86-4f87-9479-70555f897c21",
   "metadata": {},
   "source": [
    "## 3. 加载 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296518a3-fc5f-4dd5-bd64-af4982deed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    model_max_length = 512,\n",
    "    padding_side = \"left\",\n",
    "    add_eos_token = True\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d24ffab-8052-4f27-b14e-dc6e2181f877",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tokenizer(prompt,\n",
    "                   truncation = True,\n",
    "                   max_length = 512,\n",
    "                   padding = \"max_length\")\n",
    "\n",
    "result[\"labels\"] = result[\"input_ids\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c8f8fa-4ae5-4ba2-846d-ed15f761f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt =f\"\"\"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values.\n",
    "                    This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute'].\n",
    "                    The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\n",
    "                    \n",
    "                    ### Target sentence:\n",
    "                    {data_point[\"target\"]}\n",
    "                    \n",
    "                    ### Meaning representation:\n",
    "                    {data_point[\"meaning_representation\"]}\n",
    "                  \"\"\"\n",
    "    return tokenizer(full_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39551011-1bbc-4ecb-a477-1fa85a0f8aac",
   "metadata": {},
   "source": [
    "## 4. 对train和eval数据集进行tokenzier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6a506e-8172-489b-a78e-2c9d85095af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa431daa-4fab-45d1-9b1b-3ddd6c87f458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看样本\n",
    "\n",
    "print(tokenized_train_dataset[4][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c320d32f-d8fb-455e-9398-038c710476b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tokenized_train_dataset[4][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e9f81a-dfd8-4180-89bc-8d9f96ecfef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ea0167e-6949-434d-b1b8-e97f7d78251d",
   "metadata": {},
   "source": [
    "## 5. 基于 base model 进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44955ef1-739d-4006-9fda-5a63c9fc7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看 test 数据集样本\n",
    "\n",
    "print(\"目标语句: \\n\", test_dataset[1][\"target\"])\n",
    "\n",
    "print(\"意义表示: \\n\", test_dataset[1][\"meaning_representation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb03a6a-9038-481b-bc1a-4ee500fbc4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values.\n",
    "This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute'].\n",
    "The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\n",
    "\n",
    "### Target sentence:\n",
    "Earlier, you stated that you didn't have strong feelings about PlayStation's Little Big Adventure. Is your opinion true for all games which don't have multiplayer?\n",
    "\n",
    "### Meaning representation:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d049ec84-caaf-4572-822b-3e70868d1431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新初始化 tokenizer，这样它就不会添加 padding 或 eos token\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token = True,\n",
    ")\n",
    "\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 模型推理\n",
    "    result = model.generate(**model_input, max_new_tokens=256)\n",
    "    # 解码\n",
    "    result = eval_tokenizer.decode(result[0], skip_special_tokens=True)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd5672b-c1fa-40a4-ac57-04cc91fc1862",
   "metadata": {},
   "source": [
    "## 6. 配置 LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60310901-1a48-4f40-9e03-564afef842de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpoint_enable()\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ca26948-60b5-4d81-a1d0-e774d7b6bdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters():\n",
    "    '''计算训练的参数量'''\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"训练参数量：{trainable_params} || 所有参数量：{all_param} || 可训练参数量比例：{100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab1d475-09e0-49f5-8dda-aa40aa286739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印模型\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcb5062-ea27-401d-8abd-017df2c24bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r = 8,\n",
    "    lora_alpha = 16,\n",
    "    target_moduels = [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias = \"none\",\n",
    "    lora_dropout = 0.05,\n",
    "    task_type = \"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcaf80c-7dc8-47cf-8473-78e93f6bd09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印模型\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76195288-2a9a-47ff-8faa-0a0b1a22ac65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02a82474-3f80-492f-9f5f-b22564e96a81",
   "metadata": {},
   "source": [
    "## 7. 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e9e97a-cf39-49c8-b1ae-6610026885fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a4c520-17fe-47b1-98ec-2a6a654db3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"llama-3-finetune\"\n",
    "base_model_name = \"\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    # 指定要训练的模型\n",
    "    model = model,\n",
    "    # 指定训练数据集\n",
    "    train_dataset = tokenized_train_dataset,\n",
    "    # 指定验证数据集\n",
    "    eval_dataset = tokenized_val_dataset,\n",
    "    # 训练参数配置\n",
    "    args = transformers.TrainingArguments(\n",
    "        output_dir = output_dir, # 训练输出的目录\n",
    "        warmup_steps = 5, # 训练过程中的预热步骤数 \n",
    "        # 解释：\n",
    "        # warmup_steps ：在训练的初始阶段，学习率从一个较低的值逐步增加到设定的学习率。\n",
    "        #                预热步骤的作用是避免模型在一开始就收到较大的梯度更新，从而有助于稳定训练过程。\n",
    "        per_device_train_batch = 2, # 训练批次大小\n",
    "        gradient_checkpointing = True, # 是否开启梯度检查点以节省内存\n",
    "        # 解释：\n",
    "        # gradient_checkpointing ： 这是一种技术，允许在训练过程中节省显存。\n",
    "        #                          具体来说，它会在前向传播时保存某些中间结果，而不是所有中间结果，从而减少显存占用量。然后在反向传播时，必要时重新计算这些中间结果。\n",
    "        gradient_accumulation_steps = 4, # 梯度累积的步数，实际 batch size = per_device_train_batch_size * gradient_accumulation_steps\n",
    "        max_steps = 100, # 最大训练步数，1000,5000等\n",
    "        learning_rate = 2.5e-5, # 学习率\n",
    "        logging_steps = 50, # 每 50 步记录一次日志\n",
    "        bf16 = True, # 使用 bfloat16 精度进行训练\n",
    "        optim = \"paged_adamw_8bit\", # 使用8-bit的AdamW优化器\n",
    "        # 解释：\n",
    "        # paged_adamw_8bit ：这是一种优化器的实现，将参数和梯度压缩到8-bit表示，以减少内存和计算需求。\n",
    "        #                    Paged表示则是指优化器分页处理数据，以进一步优化内存使用。\n",
    "        logging_dir = \"./logs\", # 日志存储目录\n",
    "        save_strategy = \"steps\", # 模型保存策略：每隔一定步数保存一次\n",
    "        save_steps = 50, # 每50步保存一次模型检查点\n",
    "        evaluation_strategy = \"steps\", # 评估策略：每隔一定步数进行评估\n",
    "        eval_steps = 50, # 每50步进行一次评估\n",
    "        do_eval = True, # 是否在训练结束后进行评估\n",
    "        report_to = \"wandb\",\n",
    "        run_name = f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"  # W&B 运行名称，包含当前时间戳\n",
    "    ),\n",
    "    data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False), # 数据整理器\n",
    ")\n",
    "\n",
    "model.config.use_cache = False # 禁用缓存以避免警告。推理时请重新启用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9046f7-94f6-4549-902a-f79741ea6f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train() # 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30491e1-8ae8-4272-b7e4-b1653c86e948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39c7ab02-ff43-439f-a7f0-43fb4d30b677",
   "metadata": {},
   "source": [
    "## 8. 基于base model和LoRA model进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c98529a-3570-444e-9293-b6d42a1c1bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = \"auto\",\n",
    "    trust_remote_code = True,\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token = True,\n",
    "    trust_remote_code = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d81137-62d5-479f-bf48-ac47350896e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 QLoRA adapter\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "best_qlora_checkpoint = \"\"\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model,\n",
    "                                     best_qlora_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8393e5cb-1740-4b9a-9d93-ecdb72823da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values.\n",
    "This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute'].\n",
    "The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\n",
    "\n",
    "### Target sentence:\n",
    "Earlier, you stated that you didn't have strong feelings about PlayStation's Little Big Adventure. Is your opinion true for all games which don't have multiplayer?\n",
    "\n",
    "### Meaning representation:\n",
    "\"\"\"\n",
    "\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
