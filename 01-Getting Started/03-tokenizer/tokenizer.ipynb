{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer 基本使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"弱小的我也有大梦想!\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 加载与保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从HuggingFace加载，输入模型名称，即可加载对于的分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./roberta_tokenizer\\\\tokenizer_config.json',\n",
       " './roberta_tokenizer\\\\special_tokens_map.json',\n",
       " './roberta_tokenizer\\\\vocab.txt',\n",
       " './roberta_tokenizer\\\\added_tokens.json',\n",
       " './roberta_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer 保存到本地\n",
    "tokenizer.save_pretrained(\"./roberta_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='./roberta_tokenizer/', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从本地加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./roberta_tokenizer/\")\n",
    "tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 句子分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['弱', '小', '的', '我', '也', '有', '大', '梦', '想', '!']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sen)\n",
    "tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 查看词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'##8': 8156,\n",
       " '##蛊': 19084,\n",
       " '痙': 4577,\n",
       " '鍰': 7106,\n",
       " '码': 4772,\n",
       " '141': 9554,\n",
       " '顔': 7543,\n",
       " 'h2': 12212,\n",
       " 'west': 8520,\n",
       " '##ᅧ': 13474,\n",
       " '##害': 15211,\n",
       " '3mm': 12064,\n",
       " '##‿': 13509,\n",
       " '龐': 7984,\n",
       " '％': 8017,\n",
       " '淖': 3904,\n",
       " '##缕': 18412,\n",
       " '##杂': 16382,\n",
       " '##ㄞ': 13717,\n",
       " '##赛': 19669,\n",
       " '##郫': 20014,\n",
       " '##ith': 11440,\n",
       " '##珉': 17452,\n",
       " '##tle': 11283,\n",
       " '盖': 4667,\n",
       " 'general': 11568,\n",
       " '##覺': 19278,\n",
       " '##翅': 18477,\n",
       " 'focus': 11452,\n",
       " '梭': 3460,\n",
       " '##瘘': 17657,\n",
       " '酪': 6991,\n",
       " '飙': 7604,\n",
       " '##谘': 19519,\n",
       " '##鹂': 20954,\n",
       " 'ⅰ': 363,\n",
       " '顽': 7559,\n",
       " '峴': 2293,\n",
       " '邦': 6930,\n",
       " '##セ': 13686,\n",
       " '711': 12565,\n",
       " '麴': 7933,\n",
       " '##橼': 16645,\n",
       " 'netscape': 8691,\n",
       " '##lock': 11248,\n",
       " '吓': 1405,\n",
       " 'xyz': 11266,\n",
       " '嗓': 1624,\n",
       " '##cept': 11877,\n",
       " '际': 7354,\n",
       " '་': 286,\n",
       " '##art': 9528,\n",
       " '抛': 2837,\n",
       " '##host': 12227,\n",
       " '##カー': 10175,\n",
       " '1350': 12809,\n",
       " 'talk2yam': 8767,\n",
       " '##97': 9410,\n",
       " '靠': 7479,\n",
       " '##０': 8569,\n",
       " '7x24': 12486,\n",
       " '##ads': 12514,\n",
       " '##ب': 13428,\n",
       " '##鵑': 20921,\n",
       " '##厥': 14392,\n",
       " '##在': 14819,\n",
       " '##女': 15014,\n",
       " '##登': 17690,\n",
       " '悬': 2647,\n",
       " '##摟': 16095,\n",
       " '##┣': 13585,\n",
       " '鳌': 7845,\n",
       " '皖': 4646,\n",
       " '[unused2]': 2,\n",
       " 'kanshu': 13236,\n",
       " '##ot': 8783,\n",
       " '脣': 5560,\n",
       " '[unused6]': 6,\n",
       " '##et': 8418,\n",
       " '甯': 4505,\n",
       " '##網': 18263,\n",
       " '##纲': 18343,\n",
       " '##賓': 19597,\n",
       " '颈': 7568,\n",
       " 'ⅱ': 364,\n",
       " '繫': 5258,\n",
       " '##リー': 10535,\n",
       " '攝': 3109,\n",
       " '##擂': 16133,\n",
       " 'gmp': 10821,\n",
       " '販': 6516,\n",
       " '贩': 6575,\n",
       " '頓': 7524,\n",
       " 'cheese': 11387,\n",
       " 'cover': 12555,\n",
       " '##糾': 18201,\n",
       " '##才': 15855,\n",
       " '楷': 3514,\n",
       " '趣': 6637,\n",
       " '##埃': 14869,\n",
       " '##眺': 17762,\n",
       " '浪': 3857,\n",
       " '##趣': 19694,\n",
       " '##飨': 20667,\n",
       " '##賞': 19599,\n",
       " '爸': 4268,\n",
       " '##62': 9290,\n",
       " '闢': 7304,\n",
       " '霊': 7451,\n",
       " '##膛': 18662,\n",
       " '##构': 16411,\n",
       " '嘜': 1659,\n",
       " '##傍': 14045,\n",
       " '##います': 9184,\n",
       " '##咏': 14528,\n",
       " 'paypal': 8657,\n",
       " '耳': 5455,\n",
       " '188': 9460,\n",
       " 'fairmont': 12208,\n",
       " '##穎': 18006,\n",
       " '警': 6356,\n",
       " '抢': 2843,\n",
       " '##堇': 14889,\n",
       " '##便': 13969,\n",
       " 'glass': 12535,\n",
       " '俚': 923,\n",
       " '僵': 1018,\n",
       " '勁': 1233,\n",
       " '姑': 1996,\n",
       " '铮': 7209,\n",
       " '##our': 9832,\n",
       " 'ろ': 582,\n",
       " '##絹': 18248,\n",
       " '##峇': 15337,\n",
       " '##誤': 19356,\n",
       " '廿': 2457,\n",
       " 'ありな': 13022,\n",
       " '##坡': 14843,\n",
       " '##牟': 17340,\n",
       " '389': 12512,\n",
       " '々': 513,\n",
       " '屎': 2241,\n",
       " '廚': 2446,\n",
       " '貂': 6503,\n",
       " 'youtube': 8487,\n",
       " '##彆': 15548,\n",
       " '擁': 3075,\n",
       " '##钟': 20221,\n",
       " '##钰': 20234,\n",
       " '##瀞': 17168,\n",
       " '##静': 20531,\n",
       " 'cma': 12398,\n",
       " '##仗': 13858,\n",
       " '##每': 16737,\n",
       " '##万': 13731,\n",
       " '祺': 4878,\n",
       " 'あります': 12737,\n",
       " 'sci': 11776,\n",
       " '叢': 1365,\n",
       " 'adam': 11194,\n",
       " '倚': 953,\n",
       " '##咘': 14535,\n",
       " '拔': 2869,\n",
       " '##渎': 16989,\n",
       " '隘': 7394,\n",
       " '斑': 3157,\n",
       " 'd3': 10966,\n",
       " '##ㄇ': 13707,\n",
       " '##跡': 19714,\n",
       " '友': 1351,\n",
       " '##蒂': 18938,\n",
       " '枇': 3355,\n",
       " '##012': 12037,\n",
       " 'ds': 11274,\n",
       " '##鈉': 20099,\n",
       " '##轅': 19807,\n",
       " '攪': 3115,\n",
       " '脐': 5553,\n",
       " '##骜': 20805,\n",
       " '##鹅': 20957,\n",
       " '##觸': 19297,\n",
       " '2004': 8258,\n",
       " '繪': 5257,\n",
       " '##帜': 15426,\n",
       " '##疼': 17620,\n",
       " 'safari': 8580,\n",
       " '##bay': 13200,\n",
       " '诣': 6419,\n",
       " '缢': 5363,\n",
       " '従': 2531,\n",
       " '##ม': 13445,\n",
       " '##躲': 19776,\n",
       " '趁': 6630,\n",
       " '伴': 845,\n",
       " '##僖': 14068,\n",
       " '##乃': 13775,\n",
       " '肠': 5499,\n",
       " '挺': 2923,\n",
       " '##啶': 14635,\n",
       " '##pn': 12991,\n",
       " '##翰': 18489,\n",
       " '##ive': 8857,\n",
       " '同': 1398,\n",
       " '♥yoyo♥': 8948,\n",
       " '181': 10111,\n",
       " '##澧': 17133,\n",
       " '##澆': 17125,\n",
       " '##邯': 19992,\n",
       " '拡': 2878,\n",
       " 'eleven': 9795,\n",
       " 'copy': 11669,\n",
       " '拉': 2861,\n",
       " '惊': 2661,\n",
       " '##ation': 8794,\n",
       " '209': 10381,\n",
       " '##辆': 19832,\n",
       " '##cel': 11786,\n",
       " '築': 5064,\n",
       " 'amd': 8992,\n",
       " '1927': 9620,\n",
       " '378': 12770,\n",
       " '##蘭': 19041,\n",
       " '铸': 7214,\n",
       " '##更': 16348,\n",
       " '彫': 2508,\n",
       " '恰': 2623,\n",
       " '鄱': 6974,\n",
       " '##溟': 17036,\n",
       " '##负': 19623,\n",
       " '##煦': 17268,\n",
       " '碗': 4813,\n",
       " '##姦': 15063,\n",
       " '愧': 2700,\n",
       " '學': 2119,\n",
       " '##扰': 15874,\n",
       " '##撼': 16129,\n",
       " 'ш': 256,\n",
       " 'font': 11851,\n",
       " '##怎': 15639,\n",
       " '##cl': 10753,\n",
       " '##暴': 16331,\n",
       " '##圳': 14823,\n",
       " '媲': 2059,\n",
       " 'a7': 11226,\n",
       " '##蜿': 19122,\n",
       " '##迥': 19886,\n",
       " 'they': 12013,\n",
       " '##年': 15456,\n",
       " 'table': 10809,\n",
       " '##貶': 19581,\n",
       " '啟': 1564,\n",
       " '诚': 6411,\n",
       " '##穗': 18007,\n",
       " '##sol': 12257,\n",
       " '##ice': 8877,\n",
       " '##賁': 19589,\n",
       " '##阇': 20384,\n",
       " '┆': 432,\n",
       " '##吝': 14467,\n",
       " '##缅': 18404,\n",
       " '##锐': 20286,\n",
       " '囪': 1734,\n",
       " '##颗': 20635,\n",
       " 'к': 242,\n",
       " '##38': 9137,\n",
       " 'ebay': 8886,\n",
       " '##荽': 18853,\n",
       " '總': 5244,\n",
       " '##啲': 14633,\n",
       " '嵬': 2320,\n",
       " '##偕': 14032,\n",
       " '##农': 14150,\n",
       " '薄': 5946,\n",
       " 'final': 10591,\n",
       " '##廃': 15497,\n",
       " '##憚': 15790,\n",
       " 'puma': 11803,\n",
       " 'ktv': 8894,\n",
       " '颗': 7578,\n",
       " 'にこ': 13059,\n",
       " '訴': 6260,\n",
       " '##００': 9885,\n",
       " '豁': 6485,\n",
       " 'php': 8531,\n",
       " 'sunday': 11548,\n",
       " '##璎': 17522,\n",
       " '站': 4991,\n",
       " '怎': 2582,\n",
       " 'daily': 10210,\n",
       " '职': 5466,\n",
       " '##东': 13748,\n",
       " '##鏢': 20186,\n",
       " 'kitchen': 11808,\n",
       " '##颖': 20634,\n",
       " '屜': 2246,\n",
       " '坚': 1780,\n",
       " '胡': 5529,\n",
       " '##line': 8762,\n",
       " '鴕': 7858,\n",
       " '##朕': 16362,\n",
       " '##殷': 16725,\n",
       " '##煎': 17260,\n",
       " '##uy': 12574,\n",
       " 'openload': 13096,\n",
       " '##聽': 18538,\n",
       " '##鴦': 20917,\n",
       " '滸': 4019,\n",
       " '##餡': 20687,\n",
       " 'shopping': 9662,\n",
       " 'シ': 604,\n",
       " '##偈': 14027,\n",
       " '吶': 1428,\n",
       " '宁': 2123,\n",
       " '操': 3082,\n",
       " 'tcp': 9901,\n",
       " '▲topapr': 10162,\n",
       " '##id': 8601,\n",
       " '##脑': 18611,\n",
       " '##mi': 8625,\n",
       " '敗': 3134,\n",
       " '公': 1062,\n",
       " '汆': 3725,\n",
       " '−': 377,\n",
       " '菓': 5828,\n",
       " 'wang': 9660,\n",
       " '212': 10164,\n",
       " '寓': 2171,\n",
       " '##丟': 13751,\n",
       " '##詐': 19323,\n",
       " '##郤': 20010,\n",
       " '觑': 6234,\n",
       " '嘖': 1654,\n",
       " '庾': 2437,\n",
       " '邱': 6937,\n",
       " '！': 8013,\n",
       " '##葫': 18929,\n",
       " '##活': 16890,\n",
       " '##遊': 19936,\n",
       " '濟': 4089,\n",
       " '败': 6571,\n",
       " '2008': 8182,\n",
       " '诞': 6414,\n",
       " '##mn': 11919,\n",
       " '##兖': 14110,\n",
       " 'oa': 10527,\n",
       " '##诶': 19491,\n",
       " '珪': 4407,\n",
       " '虎': 5988,\n",
       " '釜': 7035,\n",
       " '##fl': 11690,\n",
       " '卞': 1302,\n",
       " '绥': 5324,\n",
       " '##吏': 14458,\n",
       " '##熄': 17276,\n",
       " '坑': 1778,\n",
       " '⋯': 403,\n",
       " '樸': 3571,\n",
       " '##母': 16735,\n",
       " '輯': 6744,\n",
       " 'g4g': 10613,\n",
       " 'のないフロクに': 10900,\n",
       " '##式': 15523,\n",
       " '」': 520,\n",
       " '致': 5636,\n",
       " '##枫': 16424,\n",
       " '##雞': 20487,\n",
       " '鲫': 7836,\n",
       " 'left': 12744,\n",
       " '##中': 13761,\n",
       " '殤': 3662,\n",
       " '符': 5016,\n",
       " '##侨': 13962,\n",
       " '捂': 2926,\n",
       " '訥': 6255,\n",
       " '980': 10536,\n",
       " 'museum': 10553,\n",
       " 'panasonic': 10752,\n",
       " '##請': 19370,\n",
       " '##针': 20208,\n",
       " '##〓': 13660,\n",
       " ':': 131,\n",
       " '习': 739,\n",
       " '##妣': 15035,\n",
       " 'keep': 11654,\n",
       " '##维': 18392,\n",
       " '##饥': 20702,\n",
       " '##淩': 16970,\n",
       " '##織': 18308,\n",
       " '廝': 2447,\n",
       " '013': 13034,\n",
       " '##aw': 10922,\n",
       " 'shop': 9926,\n",
       " '##綏': 18250,\n",
       " '潔': 4049,\n",
       " '憋': 2728,\n",
       " '葦': 5870,\n",
       " '##訂': 19299,\n",
       " '乞': 737,\n",
       " '##宰': 15210,\n",
       " '272': 11281,\n",
       " '##諳': 19383,\n",
       " '叽': 1389,\n",
       " '囂': 1716,\n",
       " '郑': 6948,\n",
       " '##嘆': 14704,\n",
       " '555': 11723,\n",
       " '##缇': 18406,\n",
       " '诏': 6405,\n",
       " '駅': 7686,\n",
       " '衬': 6137,\n",
       " '1900': 8985,\n",
       " '稹': 4939,\n",
       " '桥': 3441,\n",
       " '禦': 4888,\n",
       " 'tomtom': 12196,\n",
       " '##tra': 9808,\n",
       " '##迩': 19888,\n",
       " 'ᄉ': 296,\n",
       " '##ties': 11199,\n",
       " '##噱': 14751,\n",
       " 'hitachi': 12294,\n",
       " 'bmw': 8943,\n",
       " 'factory': 12896,\n",
       " 'adsl': 13080,\n",
       " '麝': 7928,\n",
       " '涯': 3889,\n",
       " 'vmalife': 10823,\n",
       " '诡': 6417,\n",
       " 'п': 247,\n",
       " 'airport': 11435,\n",
       " '##ough': 11600,\n",
       " '##ra': 8332,\n",
       " '##喪': 14660,\n",
       " '##3': 8152,\n",
       " '钺': 7182,\n",
       " '##片': 17332,\n",
       " 'brake': 8550,\n",
       " 'festival': 11720,\n",
       " '##蜓': 19109,\n",
       " '51': 8246,\n",
       " '悶': 2653,\n",
       " '##れる': 10273,\n",
       " '綿': 5214,\n",
       " '##ann': 12464,\n",
       " '##監': 17732,\n",
       " '##磊': 17887,\n",
       " '崙': 2306,\n",
       " '誰': 6306,\n",
       " '##bs': 9071,\n",
       " 'xxx': 8790,\n",
       " '丢': 696,\n",
       " '##伢': 13895,\n",
       " '兇': 1043,\n",
       " '闊': 7295,\n",
       " '##瓦': 17539,\n",
       " 'facebooktwitterpinterestgoogle': 11498,\n",
       " '##辍': 19837,\n",
       " '##閩': 20344,\n",
       " '##溏': 17031,\n",
       " '踵': 6683,\n",
       " '櫸': 3606,\n",
       " '##の': 8227,\n",
       " '啃': 1553,\n",
       " 'jr': 8602,\n",
       " 'vera': 9378,\n",
       " '##（': 21077,\n",
       " '##躺': 19777,\n",
       " '##兢': 14113,\n",
       " '儿': 1036,\n",
       " 'magazine': 10469,\n",
       " '251': 10924,\n",
       " '##rder': 12658,\n",
       " '##癱': 17686,\n",
       " '023': 13244,\n",
       " '嗪': 1636,\n",
       " 'bo': 11059,\n",
       " '滝': 4004,\n",
       " '##莱': 18869,\n",
       " '##薩': 19015,\n",
       " '邓': 6924,\n",
       " '##護': 19419,\n",
       " '##陂': 20409,\n",
       " '郷': 6961,\n",
       " '##瞟': 17797,\n",
       " '證': 6349,\n",
       " 'foundation': 12099,\n",
       " '##姬': 15067,\n",
       " '帷': 2381,\n",
       " '##倘': 14008,\n",
       " '熹': 4231,\n",
       " '271': 11206,\n",
       " 'cnn': 9206,\n",
       " '酯': 6994,\n",
       " '##穆': 18003,\n",
       " '証': 6264,\n",
       " '##薯': 19018,\n",
       " '##讯': 19437,\n",
       " '##丘': 13744,\n",
       " '##&': 13322,\n",
       " '##璽': 17530,\n",
       " 'structure': 13286,\n",
       " 'ˈ': 202,\n",
       " '##👍': 21124,\n",
       " '腈': 5571,\n",
       " '颌': 7571,\n",
       " '館': 7631,\n",
       " 'jane': 11909,\n",
       " '[unused71]': 71,\n",
       " '##ang': 8688,\n",
       " 'eye': 11650,\n",
       " '66': 8347,\n",
       " '输': 6783,\n",
       " '##して': 9379,\n",
       " '淞': 3908,\n",
       " '釐': 7031,\n",
       " 'в': 235,\n",
       " '規': 6211,\n",
       " 'q10': 11382,\n",
       " '##ters': 12017,\n",
       " '##學': 15176,\n",
       " '僚': 1012,\n",
       " '嚥': 1710,\n",
       " '鲤': 7834,\n",
       " 'techorz': 12389,\n",
       " '##帽': 15441,\n",
       " '##烃': 17220,\n",
       " '##邑': 19980,\n",
       " '風': 7591,\n",
       " '##麴': 20990,\n",
       " 'かかります': 9791,\n",
       " '##笼': 18078,\n",
       " '柱': 3393,\n",
       " '藜': 5970,\n",
       " '##澈': 17126,\n",
       " '##傲': 14057,\n",
       " '##寓': 15228,\n",
       " '##槁': 16594,\n",
       " 'sk': 9820,\n",
       " '##訝': 19309,\n",
       " '揽': 3005,\n",
       " '羌': 5400,\n",
       " '##綴': 18264,\n",
       " '##king': 9740,\n",
       " '299': 9600,\n",
       " '##wer': 10668,\n",
       " 'ultra': 10893,\n",
       " '焗': 4187,\n",
       " '##奕': 15002,\n",
       " '蘊': 5980,\n",
       " '##娘': 15080,\n",
       " '##op': 9133,\n",
       " '##凿': 14199,\n",
       " '翳': 5434,\n",
       " '獅': 4353,\n",
       " '##輔': 19794,\n",
       " '鲑': 7829,\n",
       " '锆': 7223,\n",
       " 'london': 9978,\n",
       " '养': 1075,\n",
       " '\"': 107,\n",
       " '傣': 994,\n",
       " '漿': 4043,\n",
       " 'balance': 11606,\n",
       " '韌': 7501,\n",
       " '##米': 18158,\n",
       " 'bar': 9054,\n",
       " '##▪': 13607,\n",
       " '##鉗': 20116,\n",
       " '扪': 2811,\n",
       " '##蝇': 19123,\n",
       " '缪': 5368,\n",
       " '厕': 1329,\n",
       " 'junior': 12552,\n",
       " '圧': 1761,\n",
       " '##甥': 17555,\n",
       " '##畏': 17576,\n",
       " '##结': 18367,\n",
       " '##ru': 10409,\n",
       " 'ferragamo': 9992,\n",
       " '卒': 1293,\n",
       " '##蔼': 18985,\n",
       " '##誇': 19345,\n",
       " 'めて': 11967,\n",
       " 'a6': 11716,\n",
       " '岘': 2267,\n",
       " '##南': 14355,\n",
       " 'works': 13112,\n",
       " '㈦': 667,\n",
       " 'lab': 11441,\n",
       " 'kim': 10683,\n",
       " '##sco': 11364,\n",
       " '##∟': 13534,\n",
       " '虫': 6001,\n",
       " '剂': 1177,\n",
       " '##fr': 13245,\n",
       " '##л': 13410,\n",
       " '蛇': 6026,\n",
       " '##伸': 13904,\n",
       " '猴': 4347,\n",
       " '##泮': 16860,\n",
       " '##炅': 17196,\n",
       " '靚': 7475,\n",
       " '氐': 3695,\n",
       " '##漁': 17079,\n",
       " '##幄': 15444,\n",
       " '##渭': 17005,\n",
       " '充': 1041,\n",
       " '##致': 18693,\n",
       " '钙': 7159,\n",
       " '##訣': 19311,\n",
       " '##wo': 10911,\n",
       " '1926': 10126,\n",
       " '濡': 4091,\n",
       " '邈': 6919,\n",
       " '##獸': 17423,\n",
       " '##03': 9042,\n",
       " 'joseph': 11151,\n",
       " '##公': 14119,\n",
       " '眞': 4695,\n",
       " '珏': 4398,\n",
       " '30': 8114,\n",
       " '215': 10082,\n",
       " '##oz': 13102,\n",
       " 'з': 240,\n",
       " '417': 13038,\n",
       " '働': 1007,\n",
       " 'cool1': 12032,\n",
       " '##太': 14979,\n",
       " '险': 7372,\n",
       " '##ube': 10957,\n",
       " '巨': 2342,\n",
       " '沾': 3783,\n",
       " '##鄲': 20032,\n",
       " '##酗': 20041,\n",
       " '##鐘': 20189,\n",
       " '##ors': 10903,\n",
       " '##婁': 15094,\n",
       " '##呸': 14516,\n",
       " '梏': 3451,\n",
       " '##0': 8129,\n",
       " 'longchamp': 9003,\n",
       " 'mcu': 10738,\n",
       " '##啞': 14620,\n",
       " '遐': 6884,\n",
       " '##槐': 16600,\n",
       " '爾': 4273,\n",
       " '件': 816,\n",
       " '##湫': 17021,\n",
       " '##紊': 18207,\n",
       " '双': 1352,\n",
       " '疲': 4558,\n",
       " '蟄': 6092,\n",
       " 'sex': 11288,\n",
       " 'wu': 10552,\n",
       " 'money': 10348,\n",
       " '##标': 16460,\n",
       " '##暄': 16315,\n",
       " '針': 7036,\n",
       " '##翡': 18486,\n",
       " '##濕': 17143,\n",
       " '奚': 1949,\n",
       " '##巫': 15401,\n",
       " '汉': 3727,\n",
       " '蔬': 5922,\n",
       " 'ces': 9818,\n",
       " 'sound': 12516,\n",
       " '##逢': 19921,\n",
       " '##饨': 20703,\n",
       " '##ヽ': 13704,\n",
       " '##治': 16837,\n",
       " 'ｗ': 8073,\n",
       " '1400': 9439,\n",
       " '##趕': 19691,\n",
       " '阉': 7329,\n",
       " '練': 5230,\n",
       " 'coc': 11917,\n",
       " '##恥': 15672,\n",
       " '##=': 13335,\n",
       " '##dm': 10420,\n",
       " '蠱': 6113,\n",
       " 'access': 11098,\n",
       " '##蔵': 18981,\n",
       " '诲': 6431,\n",
       " '饕': 7642,\n",
       " '##葩': 18928,\n",
       " '##板': 16409,\n",
       " '鞣': 7496,\n",
       " '##鞦': 20554,\n",
       " '須': 7519,\n",
       " '##今': 13848,\n",
       " 'ل': 269,\n",
       " '舜': 5658,\n",
       " '##睢': 17775,\n",
       " '##唏': 14594,\n",
       " '蕃': 5931,\n",
       " '##le': 8268,\n",
       " 'grand': 9968,\n",
       " '琪': 4427,\n",
       " '##ene': 10600,\n",
       " 'urn': 11584,\n",
       " '##飙': 20661,\n",
       " '##鹉': 20959,\n",
       " '##鸦': 20944,\n",
       " '##绿': 18401,\n",
       " '[unused80]': 80,\n",
       " 'rosie': 10487,\n",
       " '坪': 1790,\n",
       " '##炼': 17216,\n",
       " '##绡': 18378,\n",
       " '韋': 7500,\n",
       " '##ย': 13446,\n",
       " '天': 1921,\n",
       " '##愉': 15747,\n",
       " '##tant': 12028,\n",
       " '##come': 12097,\n",
       " '豆': 6486,\n",
       " 'rose': 9497,\n",
       " '##奖': 15003,\n",
       " '探': 2968,\n",
       " '##ユ': 13698,\n",
       " '##禄': 17939,\n",
       " '踟': 6676,\n",
       " '##ᄁ': 13455,\n",
       " '##夭': 14981,\n",
       " '##谁': 19500,\n",
       " '##犒': 17359,\n",
       " '##翌': 18479,\n",
       " '舫': 5662,\n",
       " '陈': 7357,\n",
       " 'aaa': 10876,\n",
       " '##蕭': 18998,\n",
       " '##飕': 20659,\n",
       " '奔': 1944,\n",
       " 'ㄛ': 658,\n",
       " '谥': 6471,\n",
       " '##吸': 14486,\n",
       " '##櫥': 16662,\n",
       " '4200': 12395,\n",
       " '##老': 18496,\n",
       " 'v5': 11133,\n",
       " '##噜': 14743,\n",
       " '##墩': 14932,\n",
       " '##蟋': 19151,\n",
       " '乏': 726,\n",
       " '##cc': 8860,\n",
       " '356': 12215,\n",
       " '蚂': 6010,\n",
       " '圻': 1768,\n",
       " '嗣': 1632,\n",
       " '##垦': 14862,\n",
       " '嗒': 1623,\n",
       " '##權': 16666,\n",
       " '##差': 15402,\n",
       " 'mp4': 9399,\n",
       " 'city': 8869,\n",
       " '##処': 14186,\n",
       " '瑯': 4455,\n",
       " '##夔': 14967,\n",
       " 'msci': 9400,\n",
       " '前': 1184,\n",
       " '293': 11855,\n",
       " '洼': 3834,\n",
       " '91': 8440,\n",
       " '绸': 5339,\n",
       " '迂': 6811,\n",
       " '##dt': 12672,\n",
       " '##倌': 14001,\n",
       " 'よ': 577,\n",
       " '##绶': 18394,\n",
       " '緋': 5216,\n",
       " '210': 9083,\n",
       " '燻': 4252,\n",
       " '##为': 13768,\n",
       " '衆': 6120,\n",
       " '##饲': 20711,\n",
       " '降': 7360,\n",
       " '##瓜': 17535,\n",
       " '##雹': 20498,\n",
       " '冊': 1084,\n",
       " '##筠': 18092,\n",
       " 'eeworld': 12490,\n",
       " '##人': 13839,\n",
       " '撲': 3067,\n",
       " 'tripadvisor': 8194,\n",
       " '##剌': 14240,\n",
       " '##瘋': 17654,\n",
       " 'ᵉ': 331,\n",
       " 'ｆ': 8056,\n",
       " '飪': 7612,\n",
       " '##六': 14120,\n",
       " '##墮': 14933,\n",
       " '勵': 1252,\n",
       " 'parker': 12495,\n",
       " '##。': 13646,\n",
       " '吾': 1434,\n",
       " '答': 5031,\n",
       " 'vsa': 10920,\n",
       " '##＆': 21075,\n",
       " '哪': 1525,\n",
       " '##颂': 20620,\n",
       " '封': 2196,\n",
       " '羸': 5415,\n",
       " '##缚': 18416,\n",
       " '##禁': 17938,\n",
       " '177': 10132,\n",
       " '滥': 4010,\n",
       " '颱': 7593,\n",
       " '紧': 5165,\n",
       " '##▌': 13603,\n",
       " '##澤': 17132,\n",
       " '撈': 3051,\n",
       " 'base': 11668,\n",
       " '##顎': 20598,\n",
       " '##頸': 20591,\n",
       " '##ing': 8221,\n",
       " '##唄': 14590,\n",
       " 'board': 12020,\n",
       " '##怕': 15643,\n",
       " '##潜': 17109,\n",
       " '##筛': 18090,\n",
       " '##树': 16466,\n",
       " '轶': 6767,\n",
       " 'site': 11215,\n",
       " '体': 860,\n",
       " 'ieee': 12272,\n",
       " '弭': 2481,\n",
       " '遮': 6902,\n",
       " '##溯': 17042,\n",
       " '##莉': 18856,\n",
       " 'ol': 8972,\n",
       " 'tila': 10396,\n",
       " '##砒': 17833,\n",
       " '##陰': 20431,\n",
       " '鎔': 7114,\n",
       " '##凳': 14192,\n",
       " '##靠': 20536,\n",
       " 'gnu': 12367,\n",
       " '腦': 5582,\n",
       " '##側': 14036,\n",
       " '##篑': 18122,\n",
       " '##飢': 20666,\n",
       " '##王': 17431,\n",
       " '##伦': 13897,\n",
       " 'l1': 12421,\n",
       " '##遙': 19948,\n",
       " 'into': 12609,\n",
       " '##饵': 20713,\n",
       " '##╮': 13592,\n",
       " '##泷': 16866,\n",
       " '##沈': 16812,\n",
       " 'されます': 12001,\n",
       " '昨': 3219,\n",
       " '##dget': 11857,\n",
       " '遷': 6907,\n",
       " '##园': 14793,\n",
       " '##秆': 17959,\n",
       " '##釵': 20097,\n",
       " '劍': 1210,\n",
       " 'union': 12161,\n",
       " '##遐': 19941,\n",
       " '佝': 869,\n",
       " '琺': 4436,\n",
       " '扔': 2803,\n",
       " '##tail': 11662,\n",
       " 'santa': 12321,\n",
       " 'jing': 13207,\n",
       " '##敝': 16195,\n",
       " '缴': 5373,\n",
       " '閱': 7288,\n",
       " '页': 7552,\n",
       " 'roger': 13033,\n",
       " '##渣': 16999,\n",
       " '##って': 9067,\n",
       " '##贝': 19621,\n",
       " '##颅': 20622,\n",
       " '汙': 3732,\n",
       " '賂': 6533,\n",
       " '##ition': 11418,\n",
       " '##饌': 20695,\n",
       " '##อ': 13448,\n",
       " '##ᄀ': 13454,\n",
       " '##寧': 15237,\n",
       " '榭': 3531,\n",
       " '##瀘': 17164,\n",
       " '##ble': 8862,\n",
       " '##菀': 18877,\n",
       " '钩': 7174,\n",
       " '##ca': 8808,\n",
       " '肃': 5484,\n",
       " '角': 6235,\n",
       " '246': 11003,\n",
       " 'say': 10114,\n",
       " 'c3': 11829,\n",
       " '444': 12876,\n",
       " 'γ': 212,\n",
       " '##毯': 16748,\n",
       " '镯': 7265,\n",
       " '##cca': 13144,\n",
       " '##ming': 10693,\n",
       " '游': 3952,\n",
       " '##ics': 9034,\n",
       " '垣': 1804,\n",
       " '面': 7481,\n",
       " '##mw': 11504,\n",
       " '##钧': 20229,\n",
       " '##冕': 14146,\n",
       " '1m': 12103,\n",
       " '熱': 4229,\n",
       " '戾': 2790,\n",
       " '##糍': 18186,\n",
       " 'his': 10163,\n",
       " '単': 1299,\n",
       " '##掺': 16039,\n",
       " '茄': 5746,\n",
       " '蹙': 6694,\n",
       " '##芜': 18754,\n",
       " '▲topoct': 10135,\n",
       " '盥': 4677,\n",
       " '逻': 6872,\n",
       " '靦': 7483,\n",
       " '08': 8142,\n",
       " '瑕': 4442,\n",
       " '靄': 7469,\n",
       " 'きを': 10995,\n",
       " '##繆': 18305,\n",
       " '疾': 4565,\n",
       " '##摹': 16101,\n",
       " '骨': 7755,\n",
       " '##车': 19813,\n",
       " '領': 7526,\n",
       " '##伽': 13907,\n",
       " '吐': 1402,\n",
       " '基': 1825,\n",
       " '羲': 5414,\n",
       " '##亞': 13822,\n",
       " '##注': 16857,\n",
       " 'ﾝ': 8097,\n",
       " '##錦': 20150,\n",
       " '##噬': 14750,\n",
       " '禺': 4894,\n",
       " '聘': 5470,\n",
       " '##ღ': 13453,\n",
       " '卡': 1305,\n",
       " '2600': 10496,\n",
       " '貿': 6530,\n",
       " '判': 1161,\n",
       " 'ib': 12487,\n",
       " '##ナー': 12005,\n",
       " 'wear': 12679,\n",
       " '鋼': 7086,\n",
       " '##墅': 14920,\n",
       " '##姝': 15060,\n",
       " '##挤': 15972,\n",
       " '398': 11425,\n",
       " '##ne': 8354,\n",
       " '趾': 6644,\n",
       " '鄙': 6968,\n",
       " '122': 9203,\n",
       " '##舍': 18707,\n",
       " '325': 10838,\n",
       " '##qi': 11451,\n",
       " '沮': 3775,\n",
       " '蓬': 5908,\n",
       " '##扯': 15873,\n",
       " '##拎': 15922,\n",
       " '##蝉': 19125,\n",
       " '##berg': 10039,\n",
       " 'sdk': 10302,\n",
       " '##瀋': 17160,\n",
       " '瓴': 4485,\n",
       " '土': 1759,\n",
       " '鱼': 7824,\n",
       " '遁': 6875,\n",
       " '##is': 8331,\n",
       " 'miacare': 11918,\n",
       " 'rx': 12342,\n",
       " '##2': 8144,\n",
       " '##简': 18099,\n",
       " '眠': 4697,\n",
       " '[unused96]': 96,\n",
       " '##妞': 15034,\n",
       " 'apr': 9011,\n",
       " '##ism': 10627,\n",
       " '15058': 12347,\n",
       " '浆': 3841,\n",
       " 'pokemon': 8934,\n",
       " '##囗': 14779,\n",
       " '犸': 4309,\n",
       " 'm8': 10914,\n",
       " '##ㄛ': 13716,\n",
       " '爍': 4256,\n",
       " '##蜆': 19103,\n",
       " '##请': 19492,\n",
       " '##摳': 16099,\n",
       " '##疯': 17613,\n",
       " 'h5': 9354,\n",
       " '##簧': 18139,\n",
       " '142': 9621,\n",
       " '##尤': 15272,\n",
       " '##諧': 19378,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 索引转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将词序列转换为id序列\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['弱', '小', '的', '我', '也', '有', '大', '梦', '想', '!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将id序列转换为token序列\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'弱 小 的 我 也 有 大 梦 想!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将token序列转换为string\n",
    "str_sen = tokenizer.convert_tokens_to_string(tokens)\n",
    "str_sen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  更便捷的实现方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将字符串转换为id序列，又称之为编码\n",
    "ids = tokenizer.encode(sen, add_special_tokens=True)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 弱 小 的 我 也 有 大 梦 想! [SEP]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将id序列转换为字符串，又称之为解码\n",
    "str_sen = tokenizer.decode(ids, skip_special_tokens=False)\n",
    "str_sen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 填充与截断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 填充\n",
    "ids = tokenizer.encode(sen, padding=\"max_length\", max_length=15)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 截断\n",
    "ids = tokenizer.encode(sen, max_length=5, truncation=True)\n",
    "ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 其他输入部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.encode(sen, padding=\"max_length\", max_length=15)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = [1 if idx != 0 else 0 for idx in ids]\n",
    "token_type_ids = [0] * len(ids)\n",
    "ids, attention_mask, token_type_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 快速调用方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.encode_plus(sen, padding=\"max_length\", max_length=15)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(sen, padding=\"max_length\", max_length=15)\n",
    "inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 处理batch数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 102], [101, 3300, 3457, 2682, 6443, 6963, 749, 679, 6629, 102], [101, 6841, 6852, 3457, 2682, 4638, 2552, 8024, 3683, 3457, 2682, 3315, 6716, 8024, 3291, 1377, 6586, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sens = [\"弱小的我也有大梦想\",\n",
    "        \"有梦想谁都了不起\",\n",
    "        \"追逐梦想的心，比梦想本身，更可贵\"]\n",
    "res = tokenizer(sens)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 34.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(1000):\n",
    "    tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 36.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = tokenizer([sen] * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='./roberta_tokenizer/', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast / Slow Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"弱小的我也有大Dreaming!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slow_tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\", use_fast=False)\n",
    "slow_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 141 ms\n",
      "Wall time: 310 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(10000):\n",
    "    fast_tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 453 ms\n",
      "Wall time: 864 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(10000):\n",
    "    slow_tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 438 ms\n",
      "Wall time: 77.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = fast_tokenizer([sen] * 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 281 ms\n",
      "Wall time: 717 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = slow_tokenizer([sen] * 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 10252, 8221, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 12), (12, 15), (15, 16), (0, 0)]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = fast_tokenizer(sen, return_offsets_mapping=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast. More information on available tokenizers at https://github.com/huggingface/transformers/pull/2674",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mslow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yuyao\\miniconda3\\envs\\transformers\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2802\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2800\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2801\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2802\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[0;32m   2803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2804\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\yuyao\\miniconda3\\envs\\transformers\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2908\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2889\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2890\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2905\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2906\u001b[0m     )\n\u001b[0;32m   2907\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m   2909\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   2910\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   2911\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2912\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2913\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   2914\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   2915\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   2916\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2917\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2918\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2919\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2920\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2921\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2922\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2923\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2924\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   2925\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   2926\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2927\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\yuyao\\miniconda3\\envs\\transformers\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2981\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2971\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2972\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2973\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2974\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2978\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2979\u001b[0m )\n\u001b[1;32m-> 2981\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_plus(\n\u001b[0;32m   2982\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   2983\u001b[0m     text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   2984\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2985\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m   2986\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   2987\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   2988\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   2989\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2990\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2991\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2992\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2993\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2994\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2995\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2996\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2997\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   2998\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   2999\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3000\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\yuyao\\miniconda3\\envs\\transformers\\lib\\site-packages\\transformers\\tokenization_utils.py:711\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    705\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    706\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not valid. Should be a string, a list/tuple of strings or a list/tuple of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    707\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m integers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    708\u001b[0m             )\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m--> 711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    712\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    713\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    714\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.PreTrainedTokenizerFast. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    715\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMore information on available tokenizers at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    716\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    717\u001b[0m     )\n\u001b[0;32m    719\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text)\n\u001b[0;32m    720\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast. More information on available tokenizers at https://github.com/huggingface/transformers/pull/2674"
     ]
    }
   ],
   "source": [
    "inputs = slow_tokenizer(sen, return_offsets_mapping=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特殊Tokenizer的加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SkyworkTokenizer(name_or_path='Skywork/Skywork-13B-base', vocab_size=65519, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 新版本的transformers（>4.34），加载 THUDM/chatglm 会报错，因此这里替换为了天宫的模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Skywork/Skywork-13B-base\", trust_remote_code=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('skywork_tokenizer\\\\tokenizer_config.json',\n",
       " 'skywork_tokenizer\\\\special_tokens_map.json',\n",
       " 'skywork_tokenizer\\\\tokenizer.model',\n",
       " 'skywork_tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"skywork_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"skywork_tokenizer\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>弱小的我也有大Dreaming!'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
